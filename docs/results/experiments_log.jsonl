{"experiment_id": "f916c86b0143", "timestamp": "2025-11-12T15:50:33.440871", "model_id": "global.anthropic.claude-sonnet-4-5-20250929-v1:0", "system_instructions": "You are a metadata curation assistant. Your task is to help improve and correct metadata \naccording to the specific instructions provided for each task. Be precise, accurate, and \nfollow the guidelines carefully.\n", "overall_metrics": {"total_samples": 114, "tasks_completed": 11, "tasks_failed": 0, "average_accuracy": null, "task_metrics": {"broadening_of_narrow_synonyms": {"total_samples": 10, "successful_runs": 0, "failed_runs": 10, "success_rate": 0.0, "average_score": null, "min_score": null, "max_score": null, "num_scored": 0}, "column_enumeration": {"total_samples": 10, "successful_runs": 0, "failed_runs": 10, "success_rate": 0.0, "average_score": null, "min_score": null, "max_score": null, "num_scored": 0}, "column_type_identification": {"total_samples": 10, "successful_runs": 0, "failed_runs": 10, "success_rate": 0.0, "average_score": null, "min_score": null, "max_score": null, "num_scored": 0}, "column_value_retrieval": {"total_samples": 10, "successful_runs": 0, "failed_runs": 10, "success_rate": 0.0, "average_score": null, "min_score": null, "max_score": null, "num_scored": 0}, "correction_of_typos": {"total_samples": 10, "successful_runs": 0, "failed_runs": 10, "success_rate": 0.0, "average_score": null, "min_score": null, "max_score": null, "num_scored": 0}, "narrowing_of_broad_synonyms": {"total_samples": 10, "successful_runs": 0, "failed_runs": 10, "success_rate": 0.0, "average_score": null, "min_score": null, "max_score": null, "num_scored": 0}, "regex_generation": {"total_samples": 14, "successful_runs": 0, "failed_runs": 14, "success_rate": 0.0, "average_score": null, "min_score": null, "max_score": null, "num_scored": 0}, "row_validation_explanation": {"total_samples": 10, "successful_runs": 0, "failed_runs": 10, "success_rate": 0.0, "average_score": null, "min_score": null, "max_score": null, "num_scored": 0}, "row_value_retrieval": {"total_samples": 10, "successful_runs": 0, "failed_runs": 10, "success_rate": 0.0, "average_score": null, "min_score": null, "max_score": null, "num_scored": 0}, "translation_of_exact_synonyms": {"total_samples": 10, "successful_runs": 0, "failed_runs": 10, "success_rate": 0.0, "average_score": null, "min_score": null, "max_score": null, "num_scored": 0}, "validation_error_counting": {"total_samples": 10, "successful_runs": 0, "failed_runs": 10, "success_rate": 0.0, "average_score": null, "min_score": null, "max_score": null, "num_scored": 0}}}}
{"experiment_id": "f916c86b0143", "timestamp": "2025-11-12T16:08:32.403937", "model_id": "global.anthropic.claude-sonnet-4-5-20250929-v1:0", "system_instructions": "You are a metadata curation assistant. Your task is to help improve and correct metadata \naccording to the specific instructions provided for each task. Be precise, accurate, and \nfollow the guidelines carefully.\n", "overall_metrics": {"total_samples": 114, "tasks_completed": 11, "tasks_failed": 0, "average_accuracy": 0.793073593073593, "task_metrics": {"broadening_of_narrow_synonyms": {"total_samples": 10, "successful_runs": 10, "failed_runs": 0, "success_rate": 1.0, "average_score": 0.6666666666666667, "min_score": 0.3333333333333333, "max_score": 1.0, "num_scored": 10}, "column_enumeration": {"total_samples": 10, "successful_runs": 10, "failed_runs": 0, "success_rate": 1.0, "average_score": 1.0, "min_score": 1.0, "max_score": 1.0, "num_scored": 10}, "column_type_identification": {"total_samples": 10, "successful_runs": 10, "failed_runs": 0, "success_rate": 1.0, "average_score": 1.0, "min_score": 1.0, "max_score": 1.0, "num_scored": 10}, "column_value_retrieval": {"total_samples": 10, "successful_runs": 10, "failed_runs": 0, "success_rate": 1.0, "average_score": 1.0, "min_score": 1.0, "max_score": 1.0, "num_scored": 10}, "correction_of_typos": {"total_samples": 10, "successful_runs": 10, "failed_runs": 0, "success_rate": 1.0, "average_score": 0.7000000000000001, "min_score": 0.6666666666666666, "max_score": 1.0, "num_scored": 10}, "narrowing_of_broad_synonyms": {"total_samples": 10, "successful_runs": 10, "failed_runs": 0, "success_rate": 1.0, "average_score": 0.2333333333333333, "min_score": 0.0, "max_score": 0.6666666666666666, "num_scored": 10}, "regex_generation": {"total_samples": 14, "successful_runs": 14, "failed_runs": 0, "success_rate": 1.0, "average_score": 0.8571428571428571, "min_score": 0.0, "max_score": 1.0, "num_scored": 14}, "row_validation_explanation": {"total_samples": 10, "successful_runs": 10, "failed_runs": 0, "success_rate": 1.0, "average_score": 1.0, "min_score": 1.0, "max_score": 1.0, "num_scored": 10}, "row_value_retrieval": {"total_samples": 10, "successful_runs": 10, "failed_runs": 0, "success_rate": 1.0, "average_score": 1.0, "min_score": 1.0, "max_score": 1.0, "num_scored": 10}, "translation_of_exact_synonyms": {"total_samples": 10, "successful_runs": 10, "failed_runs": 0, "success_rate": 1.0, "average_score": 0.5666666666666667, "min_score": 0.3333333333333333, "max_score": 1.0, "num_scored": 10}, "validation_error_counting": {"total_samples": 10, "successful_runs": 10, "failed_runs": 0, "success_rate": 1.0, "average_score": 0.7, "min_score": 0.0, "max_score": 1.0, "num_scored": 10}}}}
